{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fc80b7e1",
      "metadata": {},
      "source": [
        "# First execution\n",
        "\n",
        "If you need to select the kernel, select **.venv/bin/python**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c465616e",
      "metadata": {},
      "source": [
        "# Databricks Local Analysis\n",
        "This notebook demonstrates how to interact with the local Spark environment,\n",
        "the Hive Metastore, Delta Lake features, and the full **DBUtils Shim**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "61710bc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚡ Initializing Local Spark — Databricks 16.4 LTS + Unity Catalog Emulator\n",
            "Spark Session Active — spark, dbutils, display, sc, uc disponibles\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/02/18 09:47:37 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "\n",
        "# Asegurar que databricks_shim está en el path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
        "\n",
        "# Modo local puro: desactivar servicios Docker (S3/MinIO, PostgreSQL)\n",
        "# que no están disponibles fuera de Docker\n",
        "os.environ[\"APP_ENV\"] = \"local\"\n",
        "for key in [\"AWS_ENDPOINT_URL\", \"POSTGRES_HOST\", \"POSTGRES_PORT\",\n",
        "            \"POSTGRES_DB\", \"POSTGRES_USER\", \"POSTGRES_PASSWORD\"]:\n",
        "    os.environ.pop(key, None)\n",
        "\n",
        "from databricks_shim import inject_notebook_context\n",
        "\n",
        "inject_notebook_context(\"Notebook_Data_Analysis\")\n",
        "print(\"Spark Session Active — spark, dbutils, display, sc, uc disponibles\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b86738",
      "metadata": {},
      "source": [
        "## Crear datos de ejemplo (modo local)\n",
        "Preparamos las tablas Delta necesarias directamente en el warehouse local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "d21ef537",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Silver: /tmp/nb_demo_n_bu9hck/silver\n",
            "✓ Gold:   /tmp/nb_demo_n_bu9hck/gold\n",
            "✓ Vistas temporales: products_silver, category_summary_gold creadas\n"
          ]
        }
      ],
      "source": [
        "# ── Crear esquema y datos de ejemplo ─────────────────────────────────────────\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "from pyspark.sql.functions import upper, col, avg, count, current_timestamp, round as spark_round\n",
        "import tempfile, os\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"name\", StringType()),\n",
        "    StructField(\"price\", DoubleType()),\n",
        "    StructField(\"category\", StringType()),\n",
        "])\n",
        "\n",
        "data = [\n",
        "    (1, \"Product A\", 100.0, \"electronics\"),\n",
        "    (2, \"Product B\", 200.0, \"electronics\"),\n",
        "    (3, \"Product C\",  50.0, \"clothing\"),\n",
        "    (4, \"Product D\",  75.0, \"clothing\"),\n",
        "    (5, \"Product E\", 300.0, \"home\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "# Silver: transformación\n",
        "silver_df = df.withColumn(\"name\", upper(col(\"name\"))) \\\n",
        "              .withColumn(\"category\", upper(col(\"category\")))\n",
        "\n",
        "_tmp = tempfile.mkdtemp(prefix=\"nb_demo_\")\n",
        "silver_path = os.path.join(_tmp, \"silver\")\n",
        "gold_path = os.path.join(_tmp, \"gold\")\n",
        "\n",
        "silver_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
        "\n",
        "# Gold: resumen por categoría\n",
        "gold_df = silver_df.groupBy(\"category\").agg(\n",
        "    count(\"id\").alias(\"total_products\"),\n",
        "    spark_round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
        ")\n",
        "gold_df.write.format(\"delta\").mode(\"overwrite\").save(gold_path)\n",
        "\n",
        "# Registrar como tablas temporales para consultas\n",
        "spark.read.format(\"delta\").load(silver_path).createOrReplaceTempView(\"products_silver\")\n",
        "spark.read.format(\"delta\").load(gold_path).createOrReplaceTempView(\"category_summary_gold\")\n",
        "\n",
        "print(f\"✓ Silver: {silver_path}\")\n",
        "print(f\"✓ Gold:   {gold_path}\")\n",
        "print(\"✓ Vistas temporales: products_silver, category_summary_gold creadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18ce86f1",
      "metadata": {},
      "source": [
        "## Query Silver & Gold Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "9c786ebd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Silver Table ===\n",
            "+---+---------+-----+-----------+\n",
            "| id|     name|price|   category|\n",
            "+---+---------+-----+-----------+\n",
            "|  2|PRODUCT B|200.0|ELECTRONICS|\n",
            "|  1|PRODUCT A|100.0|ELECTRONICS|\n",
            "|  4|PRODUCT D| 75.0|   CLOTHING|\n",
            "|  3|PRODUCT C| 50.0|   CLOTHING|\n",
            "|  5|PRODUCT E|300.0|       HOME|\n",
            "+---+---------+-----+-----------+\n",
            "\n",
            "\n",
            "=== Gold Table (Category Summary) ===\n",
            "+-----------+--------------+---------+\n",
            "|   category|total_products|avg_price|\n",
            "+-----------+--------------+---------+\n",
            "|ELECTRONICS|             2|    150.0|\n",
            "|   CLOTHING|             2|     62.5|\n",
            "|       HOME|             1|    300.0|\n",
            "+-----------+--------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Silver Table ===\")\n",
        "display(spark.sql(\"SELECT * FROM products_silver\"))\n",
        "\n",
        "print(\"\\n=== Gold Table (Category Summary) ===\")\n",
        "display(spark.sql(\"SELECT * FROM category_summary_gold\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f47b91",
      "metadata": {},
      "source": [
        "## Delta Time Travel\n",
        "Access previous versions of your Bronze table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "02bc998c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version 0 (original — 2 rows):\n",
            "+---+----+-----+\n",
            "| id|name|price|\n",
            "+---+----+-----+\n",
            "|  1|   A| 10.0|\n",
            "|  2|   B| 20.0|\n",
            "+---+----+-----+\n",
            "\n",
            "Version 1 (latest — 3 rows):\n",
            "+---+----+-----+\n",
            "| id|name|price|\n",
            "+---+----+-----+\n",
            "|  1|   A| 10.0|\n",
            "|  2|   B| 20.0|\n",
            "|  3|   C| 30.0|\n",
            "+---+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Escribir un Delta table con varias versiones para demostrar time travel\n",
        "import tempfile, os\n",
        "\n",
        "delta_path = os.path.join(tempfile.mkdtemp(prefix=\"delta_tt_\"), \"products\")\n",
        "\n",
        "# Version 0\n",
        "spark.createDataFrame([(1, \"A\", 10.0), (2, \"B\", 20.0)], \"id INT, name STRING, price DOUBLE\") \\\n",
        "     .write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
        "\n",
        "# Version 1 — append\n",
        "spark.createDataFrame([(3, \"C\", 30.0)], \"id INT, name STRING, price DOUBLE\") \\\n",
        "     .write.format(\"delta\").mode(\"append\").save(delta_path)\n",
        "\n",
        "print(\"Version 0 (original — 2 rows):\")\n",
        "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path).show()\n",
        "\n",
        "print(\"Version 1 (latest — 3 rows):\")\n",
        "spark.read.format(\"delta\").load(delta_path).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e375f153",
      "metadata": {},
      "source": [
        "## Delta History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "3e5c7e9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------------+---------+\n",
            "|version|timestamp              |operation|\n",
            "+-------+-----------------------+---------+\n",
            "|1      |2026-02-18 14:47:41.034|WRITE    |\n",
            "|0      |2026-02-18 14:47:40.293|WRITE    |\n",
            "+-------+-----------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "dt = DeltaTable.forPath(spark, delta_path)\n",
        "dt.history().select(\"version\", \"timestamp\", \"operation\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e262d89",
      "metadata": {},
      "source": [
        "## DBUtils Shim — Complete Test\n",
        "Test credentials, secrets, widgets, fs, notebook, jobs.taskValues and data modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "21f72a4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "credentials.showCurrentRole: []\n",
            "secrets.get: AWS_ACCESS_KEY_ID = minioadmin\n",
            "secrets.getBytes: b'minioadmin'\n",
            "secrets.listScopes: [SecretScope(name=''), SecretScope(name='app'), SecretScope(name='application')]\n"
          ]
        }
      ],
      "source": [
        "# --- dbutils.credentials (no-op locally) ---\n",
        "print(f\"credentials.showCurrentRole: {dbutils.credentials.showCurrentRole()}\")\n",
        "\n",
        "# --- dbutils.secrets ---\n",
        "aws_key = dbutils.secrets.get(\"any_scope\", \"AWS_ACCESS_KEY_ID\")\n",
        "print(f\"secrets.get: AWS_ACCESS_KEY_ID = {aws_key}\")\n",
        "\n",
        "aws_bytes = dbutils.secrets.getBytes(\"any_scope\", \"AWS_ACCESS_KEY_ID\")\n",
        "print(f\"secrets.getBytes: {aws_bytes}\")\n",
        "\n",
        "print(f\"secrets.listScopes: {dbutils.secrets.listScopes()[:3]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "eb63dc5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "widgets.get('env'): dev\n",
            "widgets.get('region'): us-east-1\n",
            "widgets.getAll(): {'env': 'dev', 'region': 'us-east-1', 'format': 'delta'}\n",
            "After remove('format'): {'env': 'dev', 'region': 'us-east-1'}\n"
          ]
        }
      ],
      "source": [
        "# --- dbutils.widgets ---\n",
        "dbutils.widgets.text(\"env\", \"dev\", \"Environment\")\n",
        "dbutils.widgets.dropdown(\"region\", \"us-east-1\", [\"us-east-1\", \"eu-west-1\"], \"Region\")\n",
        "dbutils.widgets.combobox(\"format\", \"delta\", [\"delta\", \"parquet\", \"csv\"], \"Format\")\n",
        "\n",
        "print(f\"widgets.get('env'): {dbutils.widgets.get('env')}\")\n",
        "print(f\"widgets.get('region'): {dbutils.widgets.get('region')}\")\n",
        "print(f\"widgets.getAll(): {dbutils.widgets.getAll()}\")\n",
        "\n",
        "dbutils.widgets.remove('format')\n",
        "print(f\"After remove('format'): {dbutils.widgets.getAll()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "18c328e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fs.ls:\n",
            "  data.csv  size=22\n",
            "  hello.txt  size=22\n",
            "  subdir/  size=0\n",
            "\n",
            "fs.head: Hello from dbutils.fs!\n",
            "\n",
            "After cp:\n",
            "  data.csv\n",
            "  hello.txt\n",
            "  hello_copy.txt\n",
            "  subdir/\n",
            "\n",
            "✓ Cleaned up\n"
          ]
        }
      ],
      "source": [
        "# --- dbutils.fs (usando rutas DBFS locales) ---\n",
        "dbfs_base = \"dbfs:/tmp/fs_demo\"\n",
        "dbutils.fs.mkdirs(f\"{dbfs_base}/subdir\")\n",
        "dbutils.fs.put(f\"{dbfs_base}/hello.txt\", \"Hello from dbutils.fs!\", overwrite=True)\n",
        "dbutils.fs.put(f\"{dbfs_base}/data.csv\", \"id,name\\n1,alpha\\n2,beta\", overwrite=True)\n",
        "\n",
        "print(\"fs.ls:\")\n",
        "for f in dbutils.fs.ls(f\"{dbfs_base}/\"):\n",
        "    print(f\"  {f.name}  size={f.size}\")\n",
        "\n",
        "print(f\"\\nfs.head: {dbutils.fs.head(f'{dbfs_base}/hello.txt')}\")\n",
        "\n",
        "# cp & rm\n",
        "dbutils.fs.cp(f\"{dbfs_base}/hello.txt\", f\"{dbfs_base}/hello_copy.txt\")\n",
        "print(f\"\\nAfter cp:\")\n",
        "for f in dbutils.fs.ls(f\"{dbfs_base}/\"):\n",
        "    print(f\"  {f.name}\")\n",
        "\n",
        "dbutils.fs.rm(f\"{dbfs_base}\", recurse=True)\n",
        "print(\"\\n✓ Cleaned up\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "06d4b1d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jobs.taskValues: row_count = 42\n"
          ]
        }
      ],
      "source": [
        "# --- dbutils.jobs.taskValues ---\n",
        "dbutils.jobs.taskValues.set(\"row_count\", 42)\n",
        "val = dbutils.jobs.taskValues.get(\"my_task\", \"row_count\", debugValue=0)\n",
        "print(f\"jobs.taskValues: row_count = {val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "19a13378",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+---------+-----------------+--------+\n",
            "|summary|                id|     name|            price|category|\n",
            "+-------+------------------+---------+-----------------+--------+\n",
            "|  count|                 5|        5|                5|       5|\n",
            "|   mean|               3.0|     NULL|            145.0|    NULL|\n",
            "| stddev|1.5811388300841898|     NULL|103.6822067666386|    NULL|\n",
            "|    min|                 1|PRODUCT A|             50.0|CLOTHING|\n",
            "|    max|                 5|PRODUCT E|            300.0|    HOME|\n",
            "+-------+------------------+---------+-----------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- dbutils.data.summarize ---\n",
        "df = spark.sql(\"SELECT * FROM products_silver\")\n",
        "dbutils.data.summarize(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0489f7f",
      "metadata": {},
      "source": [
        "## Unity Catalog — Emulación completa\n",
        "Databricks Runtime 16.4 LTS incluye Unity Catalog con namespace de tres niveles  \n",
        "`catalog.schema.table`. El objeto `uc` está disponible globalmente via `inject_notebook_context`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "72ce78f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Catálogos ===\n",
            "\n",
            "Catálogos disponibles:\n",
            "  • analytics\n",
            "  • hive_metastore\n",
            "  • main\n",
            "  • staging\n",
            "\n",
            "DESCRIBE CATALOG analytics:\n",
            "+----------------+--------------------+\n",
            "|             key|               value|\n",
            "+----------------+--------------------+\n",
            "|            name|           analytics|\n",
            "|    catalog_type|             MANAGED|\n",
            "|storage_location|/home/omar/Docume...|\n",
            "|         comment|                    |\n",
            "+----------------+--------------------+\n",
            "\n",
            "Catálogo activo: main\n",
            "Cambiado a: analytics\n"
          ]
        }
      ],
      "source": [
        "# ── UC: Catálogos ────────────────────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — Catálogos ===\\n\")\n",
        "\n",
        "# Crear catálogos (soporta: IF NOT EXISTS, MANAGED LOCATION, COMMENT)\n",
        "uc.sql(\"CREATE CATALOG IF NOT EXISTS analytics COMMENT 'Catálogo analítico'\")\n",
        "uc.sql(\"CREATE CATALOG IF NOT EXISTS staging  COMMENT 'Datos en tránsito'\")\n",
        "\n",
        "# Listar catálogos\n",
        "cats = uc.list_catalogs()\n",
        "print(\"Catálogos disponibles:\")\n",
        "for c in cats:\n",
        "    print(f\"  • {c.name}\")\n",
        "\n",
        "# DESCRIBE CATALOG\n",
        "print(\"\\nDESCRIBE CATALOG analytics:\")\n",
        "uc.sql(\"DESCRIBE CATALOG analytics\").show()\n",
        "\n",
        "# Catálogo activo\n",
        "print(f\"Catálogo activo: {uc.get_current_catalog()}\")\n",
        "uc.set_current_catalog(\"analytics\")\n",
        "print(f\"Cambiado a: {uc.get_current_catalog()}\")\n",
        "uc.set_current_catalog(\"main\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "dec24200",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Schemas y namespace tres niveles ===\n",
            "\n",
            "Schemas en 'analytics' (shim UC):\n",
            "\n",
            "--- USE CATALOG / USE SCHEMA ---\n",
            "Catálogo activo: analytics\n",
            "Schema activo: bronze\n",
            "\n",
            "SHOW SCHEMAS IN analytics:\n",
            "\n",
            "Restaurado a: main.default\n",
            "\n",
            "✓ Tabla events_demo.events creada\n",
            "Consulta events_demo.events:\n",
            "+--------+-------+----------+--------------------+\n",
            "|event_id|user_id|event_type|                  ts|\n",
            "+--------+-------+----------+--------------------+\n",
            "|       2|    101|  purchase|2026-02-18 14:47:...|\n",
            "|       1|    100|     click|2026-02-18 14:47:...|\n",
            "|       3|    100|      view|2026-02-18 14:47:...|\n",
            "+--------+-------+----------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ── UC: Schemas y tres niveles ───────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — Schemas y namespace tres niveles ===\\n\")\n",
        "\n",
        "# El shim UC gestiona schemas para cualquier catálogo\n",
        "uc.create_schema(\"analytics\", \"bronze\", comment=\"Capa Bronze\", if_not_exists=True)\n",
        "uc.create_schema(\"analytics\", \"silver\", comment=\"Capa Silver\", if_not_exists=True)\n",
        "uc.create_schema(\"analytics\", \"gold\",   comment=\"Capa Gold\",   if_not_exists=True)\n",
        "\n",
        "schemas = uc.list_schemas(\"analytics\")\n",
        "print(\"Schemas en 'analytics' (shim UC):\")\n",
        "for s in schemas:\n",
        "    print(f\"  • analytics.{s.name}\")\n",
        "\n",
        "# ── USE CATALOG / USE SCHEMA ─────────────────────────────────────────────\n",
        "print(\"\\n--- USE CATALOG / USE SCHEMA ---\")\n",
        "uc.sql(\"USE CATALOG analytics\")\n",
        "print(f\"Catálogo activo: {uc.get_current_catalog()}\")\n",
        "uc.sql(\"USE SCHEMA bronze\")\n",
        "print(f\"Schema activo: {uc.get_current_schema()}\")\n",
        "\n",
        "# SHOW SCHEMAS via SQL\n",
        "print(\"\\nSHOW SCHEMAS IN analytics:\")\n",
        "for s in uc.list_schemas(\"analytics\"):\n",
        "    print(f\"  • {s.name}\")\n",
        "\n",
        "# Restaurar\n",
        "uc.sql(\"USE CATALOG main\")\n",
        "uc.sql(\"USE SCHEMA default\")\n",
        "print(f\"\\nRestaurado a: {uc.get_current_catalog()}.{uc.get_current_schema()}\")\n",
        "\n",
        "# ── Crear tabla de eventos en main.events_demo ───────────────────────────\n",
        "import shutil, pathlib\n",
        "# Limpiar directorio y tabla si existen de una ejecución previa\n",
        "_events_db = pathlib.Path(\".warehouse/main/events_demo.db\")\n",
        "if _events_db.exists():\n",
        "    shutil.rmtree(_events_db)\n",
        "\n",
        "uc.sql(\"CREATE SCHEMA IF NOT EXISTS events_demo\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS events_demo.events\")\n",
        "spark.sql(\"DROP DATABASE IF EXISTS events_demo CASCADE\")\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS events_demo\")\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE events_demo.events\n",
        "    (event_id INT, user_id INT, event_type STRING, ts TIMESTAMP)\n",
        "    USING DELTA\n",
        "\"\"\")\n",
        "print(\"\\n✓ Tabla events_demo.events creada\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    INSERT INTO events_demo.events VALUES\n",
        "    (1, 100, 'click',    current_timestamp()),\n",
        "    (2, 101, 'purchase', current_timestamp()),\n",
        "    (3, 100, 'view',     current_timestamp())\n",
        "\"\"\")\n",
        "print(\"Consulta events_demo.events:\")\n",
        "spark.sql(\"SELECT * FROM events_demo.events\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "28b7d09f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Volumes y /Volumes/ paths ===\n",
            "\n",
            "Volumes en analytics.bronze:\n",
            "+------------+-----------+-----------+-----------+--------------------+\n",
            "|catalog_name|schema_name|       name|volume_type|    storage_location|\n",
            "+------------+-----------+-----------+-----------+--------------------+\n",
            "|   analytics|     bronze|  raw_files|    MANAGED|/home/omar/Docume...|\n",
            "|   analytics|     bronze|checkpoints|    MANAGED|/home/omar/Docume...|\n",
            "+------------+-----------+-----------+-----------+--------------------+\n",
            "\n",
            "\n",
            "dbutils.fs.ls('/Volumes/analytics/bronze/raw_files/'):\n",
            "  path=/Volumes/analytics/bronze/raw_files/config.json  name=config.json  size=46\n",
            "  path=/Volumes/analytics/bronze/raw_files/data.csv  name=data.csv  size=52\n",
            "  path=/Volumes/analytics/bronze/raw_files/subdir/  name=subdir/  size=0\n",
            "\n",
            "dbutils.fs.head(/Volumes/analytics/bronze/raw_files/config.json):\n",
            "{\"source\": \"api\", \"version\": 1, \"env\": \"prod\"}\n",
            "\n",
            "Leer CSV desde /Volumes/ con Spark:\n",
            "+---+------+-----+\n",
            "| id|  name|value|\n",
            "+---+------+-----+\n",
            "|  1|item_a|  100|\n",
            "|  2|item_b|  200|\n",
            "|  3|item_c|  300|\n",
            "+---+------+-----+\n",
            "\n",
            "\n",
            "DESCRIBE VOLUME:\n",
            "+----------------+--------------------+\n",
            "|             key|               value|\n",
            "+----------------+--------------------+\n",
            "|            name|           raw_files|\n",
            "|    catalog_name|           analytics|\n",
            "|     schema_name|              bronze|\n",
            "|     volume_type|             MANAGED|\n",
            "|storage_location|/home/omar/Docume...|\n",
            "|         comment|                    |\n",
            "+----------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ── UC: Volumes y rutas /Volumes/ ────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — Volumes y /Volumes/ paths ===\\n\")\n",
        "\n",
        "# Crear volumes gestionados\n",
        "uc.sql(\"CREATE VOLUME IF NOT EXISTS analytics.bronze.raw_files    COMMENT 'Archivos crudos'\")\n",
        "uc.sql(\"CREATE VOLUME IF NOT EXISTS analytics.bronze.checkpoints  COMMENT 'Checkpoints Spark'\")\n",
        "\n",
        "# Listar volumes\n",
        "print(\"Volumes en analytics.bronze:\")\n",
        "uc.sql(\"SHOW VOLUMES IN analytics.bronze\").show()\n",
        "\n",
        "# Usar rutas /Volumes/ con dbutils.fs (igual que en Databricks real)\n",
        "vol_base = \"/Volumes/analytics/bronze/raw_files\"\n",
        "\n",
        "dbutils.fs.put(f\"{vol_base}/config.json\",\n",
        "               '{\"source\": \"api\", \"version\": 1, \"env\": \"prod\"}',\n",
        "               overwrite=True)\n",
        "dbutils.fs.put(f\"{vol_base}/data.csv\",\n",
        "               \"id,name,value\\n1,item_a,100\\n2,item_b,200\\n3,item_c,300\",\n",
        "               overwrite=True)\n",
        "dbutils.fs.mkdirs(f\"{vol_base}/subdir\")\n",
        "\n",
        "# ls() preserva el prefijo /Volumes/ (igual que Databricks)\n",
        "print(\"\\ndbutils.fs.ls('/Volumes/analytics/bronze/raw_files/'):\")\n",
        "for fi in dbutils.fs.ls(f\"{vol_base}/\"):\n",
        "    print(f\"  path={fi.path}  name={fi.name}  size={fi.size}\")\n",
        "\n",
        "# head() funciona directamente con paths /Volumes/\n",
        "print(f\"\\ndbutils.fs.head({vol_base}/config.json):\")\n",
        "print(dbutils.fs.head(f\"{vol_base}/config.json\"))\n",
        "\n",
        "# Leer CSV desde /Volumes/ con Spark\n",
        "print(\"\\nLeer CSV desde /Volumes/ con Spark:\")\n",
        "vol_path = uc.volume_path(\"analytics\", \"bronze\", \"raw_files\", \"data.csv\")\n",
        "spark.read.csv(vol_path, header=True, inferSchema=True).show()\n",
        "\n",
        "# DESCRIBE VOLUME\n",
        "print(\"\\nDESCRIBE VOLUME:\")\n",
        "uc.sql(\"DESCRIBE VOLUME analytics.bronze.raw_files\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "21ab0e8f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — DBFS paths (dbfs:/) ===\n",
            "\n",
            "dbutils.fs.ls('dbfs:/tmp/uc_demo/'):\n",
            "  path=dbfs:/tmp/uc_demo/config.json  name=config.json\n",
            "  path=dbfs:/tmp/uc_demo/hello.txt  name=hello.txt\n",
            "\n",
            "dbutils.fs.head('dbfs:/tmp/uc_demo/hello.txt'):\n",
            "Hello from DBFS!\n",
            "\n",
            "Tras fs.cp:\n",
            "  config.json\n",
            "  hello.txt\n",
            "  hello_copy.txt\n",
            "\n",
            "dbfs:/tmp/uc_demo eliminado ✓\n"
          ]
        }
      ],
      "source": [
        "# ── UC: DBFS paths ───────────────────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — DBFS paths (dbfs:/) ===\\n\")\n",
        "\n",
        "# Escribir y leer en DBFS\n",
        "dbutils.fs.put(\"dbfs:/tmp/uc_demo/hello.txt\",  \"Hello from DBFS!\", overwrite=True)\n",
        "dbutils.fs.put(\"dbfs:/tmp/uc_demo/config.json\", '{\"key\": \"value\"}', overwrite=True)\n",
        "\n",
        "print(\"dbutils.fs.ls('dbfs:/tmp/uc_demo/'):\")\n",
        "for fi in dbutils.fs.ls(\"dbfs:/tmp/uc_demo/\"):\n",
        "    print(f\"  path={fi.path}  name={fi.name}\")\n",
        "\n",
        "print(f\"\\ndbutils.fs.head('dbfs:/tmp/uc_demo/hello.txt'):\")\n",
        "print(dbutils.fs.head(\"dbfs:/tmp/uc_demo/hello.txt\"))\n",
        "\n",
        "# Copiar entre rutas DBFS\n",
        "dbutils.fs.cp(\"dbfs:/tmp/uc_demo/hello.txt\", \"dbfs:/tmp/uc_demo/hello_copy.txt\")\n",
        "print(\"\\nTras fs.cp:\")\n",
        "for fi in dbutils.fs.ls(\"dbfs:/tmp/uc_demo/\"):\n",
        "    print(f\"  {fi.name}\")\n",
        "\n",
        "# Limpiar\n",
        "dbutils.fs.rm(\"dbfs:/tmp/uc_demo\", recurse=True)\n",
        "print(\"\\ndbfs:/tmp/uc_demo eliminado ✓\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "af36602e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Tags ===\n",
            "\n",
            "[Unity] COMMENT ON TABLE main.events_demo.events: Tabla de eventos de usuario\n",
            "Tags de main.events_demo.events:\n",
            "+--------+--------------------+\n",
            "|tag_name|           tag_value|\n",
            "+--------+--------------------+\n",
            "|     env|                prod|\n",
            "|    team|            data-eng|\n",
            "|_comment|Tabla de eventos ...|\n",
            "|     pii|               false|\n",
            "+--------+--------------------+\n",
            "\n",
            "Tags de analytics.bronze.raw_files (Volume):\n",
            "+--------+---------+\n",
            "|tag_name|tag_value|\n",
            "+--------+---------+\n",
            "|  source|      api|\n",
            "|  format|     json|\n",
            "+--------+---------+\n",
            "\n",
            "Python API get_tags: {'env': 'prod', 'team': 'data-eng', '_comment': 'Tabla de eventos de usuario', 'pii': 'false'}\n",
            "\n",
            "Tras UNSET TAGS ('pii'):\n",
            "{'env': 'prod', 'team': 'data-eng', '_comment': 'Tabla de eventos de usuario'}\n"
          ]
        }
      ],
      "source": [
        "# ── UC: Tags ─────────────────────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — Tags ===\\n\")\n",
        "\n",
        "# SET TAGS en tabla y volume\n",
        "uc.sql(\"ALTER TABLE main.events_demo.events  SET TAGS ('env'='prod', 'team'='data-eng', 'pii'='false')\")\n",
        "uc.sql(\"ALTER VOLUME analytics.bronze.raw_files SET TAGS ('source'='api', 'format'='json')\")\n",
        "\n",
        "# COMMENT ON almacenado como tag _comment\n",
        "uc.sql(\"COMMENT ON TABLE main.events_demo.events IS 'Tabla de eventos de usuario'\")\n",
        "\n",
        "# SHOW TAGS\n",
        "print(\"Tags de main.events_demo.events:\")\n",
        "uc.sql(\"SHOW TAGS ON TABLE main.events_demo.events\").show()\n",
        "\n",
        "print(\"Tags de analytics.bronze.raw_files (Volume):\")\n",
        "uc.sql(\"SHOW TAGS ON VOLUME analytics.bronze.raw_files\").show()\n",
        "\n",
        "# GET tags via Python API\n",
        "tags = uc.get_tags(\"main.events_demo.events\")\n",
        "print(f\"Python API get_tags: {tags}\")\n",
        "\n",
        "# UNSET TAGS\n",
        "uc.sql(\"ALTER TABLE main.events_demo.events UNSET TAGS ('pii')\")\n",
        "print(\"\\nTras UNSET TAGS ('pii'):\")\n",
        "print(uc.get_tags(\"main.events_demo.events\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "a9df7662",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Permisos ===\n",
            "\n",
            "[Unity] GRANT CREATE CATALOG ON METASTORE  TO data_engineers\n",
            "  ✓ GRANT CREATE CATALOG ON METASTORE TO data_engineers\n",
            "[Unity] GRANT USE CATALOG ON CATALOG analytics TO analysts\n",
            "  ✓ GRANT USE CATALOG ON CATALOG analytics TO analysts\n",
            "[Unity] GRANT USE SCHEMA ON SCHEMA analytics.bronze TO analysts\n",
            "  ✓ GRANT USE SCHEMA ON SCHEMA analytics.bronze TO analysts\n",
            "[Unity] GRANT SELECT ON TABLE main.events_demo.events TO analyst@co.com\n",
            "  ✓ GRANT SELECT ON TABLE main.events_demo.events TO analyst@co.com\n",
            "[Unity] GRANT READ VOLUME ON VOLUME analytics.bronze.raw_files TO analyst@co.com\n",
            "  ✓ GRANT READ VOLUME ON VOLUME analytics.bronze.raw_files TO analyst@co.com\n",
            "[Unity] GRANT EXECUTE ON MODEL analytics.ml.churn_model TO ml_team\n",
            "  ✓ GRANT EXECUTE ON MODEL analytics.ml.churn_model TO ml_team\n",
            "\n",
            "SHOW GRANTS (todos):\n",
            "+--------------+--------------+-----------+--------------------+\n",
            "|     principal|     privilege|object_type|          object_key|\n",
            "+--------------+--------------+-----------+--------------------+\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "|      analysts|    USE SCHEMA|     SCHEMA|    analytics.bronze|\n",
            "|analyst@co.com|   READ VOLUME|     VOLUME|analytics.bronze....|\n",
            "|       ml_team|       EXECUTE|      MODEL|analytics.ml.chur...|\n",
            "|   viewer_role|   DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "|      analysts|    USE SCHEMA|     SCHEMA|    analytics.bronze|\n",
            "|analyst@co.com|   READ VOLUME|     VOLUME|analytics.bronze....|\n",
            "|       ml_team|       EXECUTE|      MODEL|analytics.ml.chur...|\n",
            "|   viewer_role|   DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "|      analysts|    USE SCHEMA|     SCHEMA|    analytics.bronze|\n",
            "|analyst@co.com|   READ VOLUME|     VOLUME|analytics.bronze....|\n",
            "|       ml_team|       EXECUTE|      MODEL|analytics.ml.chur...|\n",
            "|   viewer_role|   DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "+--------------+--------------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "SHOW GRANTS ON TABLE main.events_demo.events:\n",
            "+--------------+-----------+-----------+--------------------+\n",
            "|     principal|  privilege|object_type|          object_key|\n",
            "+--------------+-----------+-----------+--------------------+\n",
            "|   viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|   viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|   viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|analyst@co.com|     SELECT|      TABLE|main.events_demo....|\n",
            "+--------------+-----------+-----------+--------------------+\n",
            "\n",
            "[Unity] REVOKE SELECT ON TABLE main.events_demo.events FROM analyst@co.com\n",
            "Tras REVOKE SELECT:\n",
            "+-----------+-----------+-----------+--------------------+\n",
            "|  principal|  privilege|object_type|          object_key|\n",
            "+-----------+-----------+-----------+--------------------+\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "+-----------+-----------+-----------+--------------------+\n",
            "\n",
            "[Unity] DENY INSERT ON TABLE main.events_demo.events TO viewer_role\n",
            "Tras DENY INSERT:\n",
            "+-----------+-----------+-----------+--------------------+\n",
            "|  principal|  privilege|object_type|          object_key|\n",
            "+-----------+-----------+-----------+--------------------+\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|viewer_role|DENY:INSERT|      TABLE|main.events_demo....|\n",
            "+-----------+-----------+-----------+--------------------+\n",
            "\n",
            "Audit log:\n",
            "+--------------+--------------+-------------------+--------------------------+--------------------------------+\n",
            "|user_identity |action_name   |request_object_type|request_object_name       |event_time                      |\n",
            "+--------------+--------------+-------------------+--------------------------+--------------------------------+\n",
            "|data_engineers|CREATE CATALOG|METASTORE          |                          |2026-02-18T14:43:28.067681+00:00|\n",
            "|analysts      |USE CATALOG   |CATALOG            |analytics                 |2026-02-18T14:43:28.067707+00:00|\n",
            "|analysts      |USE SCHEMA    |SCHEMA             |analytics.bronze          |2026-02-18T14:43:28.067723+00:00|\n",
            "|analyst@co.com|READ VOLUME   |VOLUME             |analytics.bronze.raw_files|2026-02-18T14:43:28.067747+00:00|\n",
            "|ml_team       |EXECUTE       |MODEL              |analytics.ml.churn_model  |2026-02-18T14:43:28.067758+00:00|\n",
            "+--------------+--------------+-------------------+--------------------------+--------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ── UC: GRANT / REVOKE / SHOW GRANTS ────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — Permisos ===\\n\")\n",
        "\n",
        "# GRANTs en distintos tipos de objetos (incluyendo METASTORE, nuevos tipos)\n",
        "grants_sql = [\n",
        "    \"GRANT CREATE CATALOG ON METASTORE TO data_engineers\",\n",
        "    \"GRANT USE CATALOG ON CATALOG analytics TO analysts\",\n",
        "    \"GRANT USE SCHEMA ON SCHEMA analytics.bronze TO analysts\",\n",
        "    \"GRANT SELECT ON TABLE main.events_demo.events TO analyst@co.com\",\n",
        "    \"GRANT READ VOLUME ON VOLUME analytics.bronze.raw_files TO analyst@co.com\",\n",
        "    \"GRANT EXECUTE ON MODEL analytics.ml.churn_model TO ml_team\",\n",
        "]\n",
        "\n",
        "for sql in grants_sql:\n",
        "    uc.sql(sql)\n",
        "    print(f\"  ✓ {sql}\")\n",
        "\n",
        "# SHOW GRANTS — todos\n",
        "print(\"\\nSHOW GRANTS (todos):\")\n",
        "uc.sql(\"SHOW GRANTS\").show()\n",
        "\n",
        "# SHOW GRANTS — filtrado por objeto\n",
        "print(\"SHOW GRANTS ON TABLE main.events_demo.events:\")\n",
        "uc.sql(\"SHOW GRANTS ON TABLE main.events_demo.events\").show()\n",
        "\n",
        "# REVOKE\n",
        "uc.sql(\"REVOKE SELECT ON TABLE main.events_demo.events FROM analyst@co.com\")\n",
        "print(\"Tras REVOKE SELECT:\")\n",
        "uc.sql(\"SHOW GRANTS ON TABLE main.events_demo.events\").show()\n",
        "\n",
        "# DENY\n",
        "uc.sql(\"DENY INSERT ON TABLE main.events_demo.events TO viewer_role\")\n",
        "print(\"Tras DENY INSERT:\")\n",
        "uc.sql(\"SHOW GRANTS ON TABLE main.events_demo.events\").show()\n",
        "\n",
        "# Audit log\n",
        "print(\"Audit log:\")\n",
        "uc.audit_log().show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "2dad6f9e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — information_schema ===\n",
            "\n",
            "Catálogos:\n",
            "+--------------+-------+\n",
            "|  catalog_name|comment|\n",
            "+--------------+-------+\n",
            "|     analytics|       |\n",
            "|hive_metastore|       |\n",
            "|          main|       |\n",
            "|       staging|       |\n",
            "+--------------+-------+\n",
            "\n",
            "Schemas en 'analytics':\n",
            "+------------+-----------+-------+\n",
            "|catalog_name|schema_name|comment|\n",
            "+------------+-----------+-------+\n",
            "+------------+-----------+-------+\n",
            "\n",
            "Tablas en 'main':\n",
            "+-------------+------------+--------------------+----------+\n",
            "|table_catalog|table_schema|          table_name|table_type|\n",
            "+-------------+------------+--------------------+----------+\n",
            "|         main|     default|category_summary_...|   MANAGED|\n",
            "|         main|     default|     products_silver|   MANAGED|\n",
            "+-------------+------------+--------------------+----------+\n",
            "\n",
            "Volumes en 'analytics':\n",
            "+--------------+-------------+-----------+-----------+--------------------+\n",
            "|volume_catalog|volume_schema|volume_name|volume_type|    storage_location|\n",
            "+--------------+-------------+-----------+-----------+--------------------+\n",
            "|     analytics|       bronze|  raw_files|    MANAGED|/home/omar/Docume...|\n",
            "|     analytics|       bronze|checkpoints|    MANAGED|/home/omar/Docume...|\n",
            "+--------------+-------------+-----------+-----------+--------------------+\n",
            "\n",
            "Table Privileges:\n",
            "+--------------+--------------+-----------+--------------------+\n",
            "|       grantee|privilege_type|object_type|         object_name|\n",
            "+--------------+--------------+-----------+--------------------+\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "|      analysts|    USE SCHEMA|     SCHEMA|    analytics.bronze|\n",
            "|analyst@co.com|   READ VOLUME|     VOLUME|analytics.bronze....|\n",
            "|       ml_team|       EXECUTE|      MODEL|analytics.ml.chur...|\n",
            "|   viewer_role|   DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "|      analysts|    USE SCHEMA|     SCHEMA|    analytics.bronze|\n",
            "|analyst@co.com|   READ VOLUME|     VOLUME|analytics.bronze....|\n",
            "|       ml_team|       EXECUTE|      MODEL|analytics.ml.chur...|\n",
            "|   viewer_role|   DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "|      analysts|    USE SCHEMA|     SCHEMA|    analytics.bronze|\n",
            "|analyst@co.com|   READ VOLUME|     VOLUME|analytics.bronze....|\n",
            "|       ml_team|       EXECUTE|      MODEL|analytics.ml.chur...|\n",
            "|   viewer_role|   DENY:INSERT|      TABLE|main.events_demo....|\n",
            "|data_engineers|CREATE CATALOG|  METASTORE|                    |\n",
            "|      analysts|   USE CATALOG|    CATALOG|           analytics|\n",
            "+--------------+--------------+-----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Routines (funciones):\n",
            "+---------------+--------------+------------+--------------------+\n",
            "|routine_catalog|routine_schema|routine_name|  routine_definition|\n",
            "+---------------+--------------+------------+--------------------+\n",
            "|           main|       default|  clean_text|Limpia texto: tri...|\n",
            "+---------------+--------------+------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ── UC: information_schema ───────────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — information_schema ===\\n\")\n",
        "\n",
        "print(\"Catálogos:\")\n",
        "uc.information_schema.catalogs().show()\n",
        "\n",
        "print(\"Schemas en 'analytics':\")\n",
        "uc.information_schema.schemata(\"analytics\").show()\n",
        "\n",
        "print(\"Tablas en 'main':\")\n",
        "uc.information_schema.tables(\"main\").show()\n",
        "\n",
        "print(\"Volumes en 'analytics':\")\n",
        "uc.information_schema.volumes(\"analytics\").show()\n",
        "\n",
        "print(\"Table Privileges:\")\n",
        "uc.information_schema.table_privileges().show()\n",
        "\n",
        "print(\"Routines (funciones):\")\n",
        "uc.information_schema.routines().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b4476c",
      "metadata": {},
      "source": [
        "## UC: Funciones, Grupos, Lineage, UNDROP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "f43dd297",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Funciones ===\n",
            "\n",
            "[Unity] Función 'main.default.calc_tax' registrada.\n",
            "Funciones en main.default:\n",
            "  • main.default.clean_text — Limpia texto: trim + lowercase\n",
            "  • main.default.calc_tax — Calcula IVA 21%\n",
            "\n",
            "DESCRIBE FUNCTION main.default.clean_text:\n",
            "+-----------+--------------------+\n",
            "|  info_name|          info_value|\n",
            "+-----------+--------------------+\n",
            "|   Function|main.default.clea...|\n",
            "|       Type|              SCALAR|\n",
            "|Description|Limpia texto: tri...|\n",
            "| Definition|  TRIM(LOWER(input))|\n",
            "+-----------+--------------------+\n",
            "\n",
            "[Unity] Función 'main.default.calc_tax' eliminada.\n",
            "\n",
            "Tras DROP calc_tax: ['clean_text']\n",
            "\n",
            "=== UNITY CATALOG — Grupos ===\n",
            "\n",
            "[Unity] 'alice@company.com' añadido al grupo 'data_engineers'.\n",
            "[Unity] 'bob@company.com' añadido al grupo 'data_engineers'.\n",
            "[Unity] 'carol@company.com' añadido al grupo 'analysts'.\n",
            "SHOW GROUPS:\n",
            "+--------------+\n",
            "|          name|\n",
            "+--------------+\n",
            "|      analysts|\n",
            "|data_engineers|\n",
            "+--------------+\n",
            "\n",
            "  Grupo 'analysts': ['carol@company.com']\n",
            "  Grupo 'data_engineers': ['alice@company.com', 'bob@company.com']\n",
            "[Unity] 'bob@company.com' eliminado del grupo 'data_engineers'.\n",
            "\n",
            "Tras REMOVE bob de data_engineers:\n",
            "  data_engineers: ['alice@company.com']\n",
            "\n",
            "=== UNITY CATALOG — UNDROP TABLE ===\n",
            "\n",
            "Tablas dropped registradas:\n",
            "+-------+------+------------+--------------------+\n",
            "|catalog|schema|   tableName|           droppedAt|\n",
            "+-------+------+------------+--------------------+\n",
            "|   main| sales|temp_staging|2026-02-18T14:43:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "|   main| sales|  old_report|2026-02-18T14:47:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "+-------+------+------------+--------------------+\n",
            "\n",
            "[Unity] UNDROP TABLE 'old_report' — restaurada.\n",
            "Tras UNDROP old_report:\n",
            "+-------+------+------------+--------------------+\n",
            "|catalog|schema|   tableName|           droppedAt|\n",
            "+-------+------+------------+--------------------+\n",
            "|   main| sales|temp_staging|2026-02-18T14:43:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "|   main| sales|temp_staging|2026-02-18T14:47:...|\n",
            "+-------+------+------------+--------------------+\n",
            "\n",
            "\n",
            "=== UNITY CATALOG — Lineage ===\n",
            "\n",
            "Lineage completo:\n",
            "+--------------------+--------------------+------------+--------------------+\n",
            "|        source_table|        target_table|lineage_type|          tracked_at|\n",
            "+--------------------+--------------------+------------+--------------------+\n",
            "|main.bronze.raw_p...|main.silver.produ...|       TABLE|2026-02-18T14:43:...|\n",
            "|main.silver.produ...|main.gold.categor...|       TABLE|2026-02-18T14:43:...|\n",
            "|main.bronze.raw_e...|main.silver.event...|       TABLE|2026-02-18T14:43:...|\n",
            "|main.bronze.raw_p...|main.silver.produ...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.silver.produ...|main.gold.categor...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_e...|main.silver.event...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_p...|main.silver.produ...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.silver.produ...|main.gold.categor...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_e...|main.silver.event...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_p...|main.silver.produ...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.silver.produ...|main.gold.categor...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_e...|main.silver.event...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_p...|main.silver.produ...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.silver.produ...|main.gold.categor...|       TABLE|2026-02-18T14:47:...|\n",
            "|main.bronze.raw_e...|main.silver.event...|       TABLE|2026-02-18T14:47:...|\n",
            "+--------------------+--------------------+------------+--------------------+\n",
            "\n",
            "Lineage filtrado por 'main.silver.products_silver':\n",
            "  main.bronze.raw_products → main.silver.products_silver (TABLE)\n",
            "  main.silver.products_silver → main.gold.category_summary (TABLE)\n",
            "  main.bronze.raw_products → main.silver.products_silver (TABLE)\n",
            "  main.silver.products_silver → main.gold.category_summary (TABLE)\n",
            "  main.bronze.raw_products → main.silver.products_silver (TABLE)\n",
            "  main.silver.products_silver → main.gold.category_summary (TABLE)\n",
            "  main.bronze.raw_products → main.silver.products_silver (TABLE)\n",
            "  main.silver.products_silver → main.gold.category_summary (TABLE)\n",
            "  main.bronze.raw_products → main.silver.products_silver (TABLE)\n",
            "  main.silver.products_silver → main.gold.category_summary (TABLE)\n"
          ]
        }
      ],
      "source": [
        "# ── UC: Funciones ─────────────────────────────────────────────────────────────\n",
        "print(\"=== UNITY CATALOG — Funciones ===\\n\")\n",
        "\n",
        "# Crear funciones via API\n",
        "uc.create_function(\"main\", \"default\", \"clean_text\",\n",
        "                   definition=\"TRIM(LOWER(input))\",\n",
        "                   description=\"Limpia texto: trim + lowercase\",\n",
        "                   if_not_exists=True)\n",
        "uc.create_function(\"main\", \"default\", \"calc_tax\",\n",
        "                   definition=\"price * 0.21\",\n",
        "                   description=\"Calcula IVA 21%\",\n",
        "                   if_not_exists=True)\n",
        "\n",
        "# Listar funciones\n",
        "funcs = uc.list_functions(\"main\", \"default\")\n",
        "print(\"Funciones en main.default:\")\n",
        "for f in funcs:\n",
        "    print(f\"  • {f.catalog_name}.{f.schema_name}.{f.name} — {f.description}\")\n",
        "\n",
        "# Describe función\n",
        "print(\"\\nDESCRIBE FUNCTION main.default.clean_text:\")\n",
        "uc.describe_function_sql(\"main.default.clean_text\").show()\n",
        "\n",
        "# Drop función\n",
        "uc.drop_function(\"main\", \"default\", \"calc_tax\")\n",
        "print(f\"\\nTras DROP calc_tax: {[f.name for f in uc.list_functions('main', 'default')]}\")\n",
        "\n",
        "# ── UC: Grupos ────────────────────────────────────────────────────────────────\n",
        "print(\"\\n=== UNITY CATALOG — Grupos ===\\n\")\n",
        "\n",
        "uc.sql(\"CREATE GROUP IF NOT EXISTS data_engineers\")\n",
        "uc.sql(\"CREATE GROUP IF NOT EXISTS analysts\")\n",
        "uc.sql(\"ALTER GROUP data_engineers ADD USER alice@company.com\")\n",
        "uc.sql(\"ALTER GROUP data_engineers ADD USER bob@company.com\")\n",
        "uc.sql(\"ALTER GROUP analysts ADD USER carol@company.com\")\n",
        "\n",
        "print(\"SHOW GROUPS:\")\n",
        "uc.sql(\"SHOW GROUPS\").show()\n",
        "\n",
        "groups = uc.list_groups()\n",
        "for g in groups:\n",
        "    print(f\"  Grupo '{g.name}': {g.members}\")\n",
        "\n",
        "# Remove member\n",
        "uc.sql(\"ALTER GROUP data_engineers REMOVE USER bob@company.com\")\n",
        "print(\"\\nTras REMOVE bob de data_engineers:\")\n",
        "for g in uc.list_groups():\n",
        "    if g.name == \"data_engineers\":\n",
        "        print(f\"  {g.name}: {g.members}\")\n",
        "\n",
        "# ── UC: UNDROP TABLE ─────────────────────────────────────────────────────────\n",
        "print(\"\\n=== UNITY CATALOG — UNDROP TABLE ===\\n\")\n",
        "\n",
        "# Simular drop y undrop\n",
        "uc.track_drop_table(\"main.sales.old_report\")\n",
        "uc.track_drop_table(\"main.sales.temp_staging\")\n",
        "print(\"Tablas dropped registradas:\")\n",
        "uc.sql(\"SHOW TABLES DROPPED\").show()\n",
        "\n",
        "uc.sql(\"UNDROP TABLE old_report\")\n",
        "print(\"Tras UNDROP old_report:\")\n",
        "uc.sql(\"SHOW TABLES DROPPED\").show()\n",
        "\n",
        "# ── UC: Lineage ──────────────────────────────────────────────────────────────\n",
        "print(\"\\n=== UNITY CATALOG — Lineage ===\\n\")\n",
        "\n",
        "uc.track_lineage(\"main.bronze.raw_products\", \"main.silver.products_silver\", \"TABLE\")\n",
        "uc.track_lineage(\"main.silver.products_silver\", \"main.gold.category_summary\", \"TABLE\")\n",
        "uc.track_lineage(\"main.bronze.raw_events\", \"main.silver.events_clean\", \"TABLE\")\n",
        "\n",
        "print(\"Lineage completo:\")\n",
        "uc.lineage_as_dataframe().show()\n",
        "\n",
        "print(\"Lineage filtrado por 'main.silver.products_silver':\")\n",
        "lineage = uc.get_lineage(\"main.silver.products_silver\")\n",
        "for l in lineage:\n",
        "    print(f\"  {l.source} → {l.target} ({l.lineage_type})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "cd498595",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== UNITY CATALOG — Comandos no-op (infraestructura cloud) ===\n",
            "\n",
            "[Unity] CREATE SHARE — no-op locally\n",
            "  ✓ CREATE SHARE data_share\n",
            "[Unity] CREATE RECIPIENT — no-op locally\n",
            "  ✓ CREATE RECIPIENT external_partner\n",
            "[Unity] CREATE EXTERNAL LOCATION — no-op locally\n",
            "  ✓ CREATE EXTERNAL LOCATION s3_location\n",
            "[Unity] CREATE STORAGE CREDENTIAL — no-op locally\n",
            "  ✓ CREATE STORAGE CREDENTIAL s3_cred\n",
            "[Unity] CREATE SERVICE CREDENTIAL — no-op locally\n",
            "  ✓ CREATE SERVICE CREDENTIAL azure_cred\n",
            "[Unity] CREATE CONNECTION — no-op locally\n",
            "  ✓ CREATE CONNECTION pg_conn\n",
            "[Unity] CREATE CLEAN ROOM — no-op locally\n",
            "  ✓ CREATE CLEAN ROOM collab_room\n",
            "[Unity] CREATE MATERIALIZED VIEW — no-op locally\n",
            "  ✓ CREATE MATERIALIZED VIEW mv_test AS SELECT 1\n",
            "[Unity] CREATE STREAMING TABLE — no-op locally\n",
            "  ✓ CREATE STREAMING TABLE st_test AS SELECT 1\n",
            "[Unity] CREATE SERVER — no-op locally\n",
            "  ✓ CREATE SERVER my_server\n",
            "[Unity] SHOW SHARES — no-op locally\n",
            "  ✓ SHOW SHARES\n",
            "[Unity] SHOW EXTERNAL LOCATIONS — no-op locally\n",
            "  ✓ SHOW EXTERNAL LOCATIONS\n",
            "[Unity] SHOW STORAGE CREDENTIALS — no-op locally\n",
            "  ✓ SHOW STORAGE CREDENTIALS\n",
            "[Unity] SHOW SERVICE CREDENTIALS — no-op locally\n",
            "  ✓ SHOW SERVICE CREDENTIALS\n",
            "[Unity] SHOW CONNECTIONS — no-op locally\n",
            "  ✓ SHOW CONNECTIONS\n",
            "[Unity] SHOW CLEAN ROOMS — no-op locally\n",
            "  ✓ SHOW CLEAN ROOMS\n",
            "[Unity] SYNC — no-op locally\n",
            "  ✓ SYNC SCHEMA my_schema\n",
            "[Unity] MSCK REPAIR PRIVILEGES — no-op locally\n",
            "  ✓ MSCK REPAIR PRIVILEGES\n",
            "[Unity] REFRESH FOREIGN — no-op locally\n",
            "  ✓ REFRESH FOREIGN CATALOG ext_cat\n",
            "\n",
            "✅ Todos los 19 no-op completados sin error\n"
          ]
        }
      ],
      "source": [
        "# ── UC: No-ops (Delta Sharing, External Locations, Credentials, etc.) ─────\n",
        "print(\"=== UNITY CATALOG — Comandos no-op (infraestructura cloud) ===\\n\")\n",
        "\n",
        "noop_commands = [\n",
        "    \"CREATE SHARE data_share\",\n",
        "    \"CREATE RECIPIENT external_partner\",\n",
        "    \"CREATE EXTERNAL LOCATION s3_location\",\n",
        "    \"CREATE STORAGE CREDENTIAL s3_cred\",\n",
        "    \"CREATE SERVICE CREDENTIAL azure_cred\",\n",
        "    \"CREATE CONNECTION pg_conn\",\n",
        "    \"CREATE CLEAN ROOM collab_room\",\n",
        "    \"CREATE MATERIALIZED VIEW mv_test AS SELECT 1\",\n",
        "    \"CREATE STREAMING TABLE st_test AS SELECT 1\",\n",
        "    \"CREATE SERVER my_server\",\n",
        "    \"SHOW SHARES\",\n",
        "    \"SHOW EXTERNAL LOCATIONS\",\n",
        "    \"SHOW STORAGE CREDENTIALS\",\n",
        "    \"SHOW SERVICE CREDENTIALS\",\n",
        "    \"SHOW CONNECTIONS\",\n",
        "    \"SHOW CLEAN ROOMS\",\n",
        "    \"SYNC SCHEMA my_schema\",\n",
        "    \"MSCK REPAIR PRIVILEGES\",\n",
        "    \"REFRESH FOREIGN CATALOG ext_cat\",\n",
        "]\n",
        "\n",
        "for cmd in noop_commands:\n",
        "    result = uc.sql(cmd)\n",
        "    print(f\"  ✓ {cmd}\")\n",
        "\n",
        "print(f\"\\n✅ Todos los {len(noop_commands)} no-op completados sin error\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "databricks-docker",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
